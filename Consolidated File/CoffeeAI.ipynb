{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee Bean Identifier using Neural Networks\n",
    "\n",
    "This notebook would serve as a consolidated file of the different resources utilized within this project. While this has been proved to be working and functioning, it is important to take note that the current state of this model is hence, only for **Proof of Concept purposes**. Further Exploratory Data Analysis, Feature Engineering, and other data activities are advised to achieve optimal and accurate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the creation of this model, there are various neural network models used to perform the identification of data. These are\n",
    "#### > VGG19 <br>\n",
    "![VGG 19 Architecture](vgg-19.png)\n",
    " - It is a deep convolutional neural network architecture proposed by the Visual Geometry Group at the University of Oxford. VGG19 consists of 19 layers, including 16 convolutional layers and 3 fully connected layers. The network is known for its simplicity and uniformity, where each layer in the network uses a small 3x3 convolutional kernel and 2x2 max-pooling layers to process the input images. VGG19 has been widely used as a benchmark for image recognition tasks due to its straightforward architecture.\n",
    "#### > ResNet50 <br>\n",
    "![Resnet50 Architecture](resnet50.ppm)\n",
    " - ResNet50, short for Residual Network 50, is a variant of the ResNet architecture introduced by Microsoft Research. ResNet is based on the concept of residual learning, which tackles the vanishing gradient problem in very deep neural networks. ResNet50 specifically refers to a ResNet with 50 layers. The network introduces skip connections, also known as shortcut connections, that enable the network to learn the residual between the input and output of each layer, making it easier to train extremely deep networks. ResNet50 is a popular choice for image classification and other computer vision tasks due to its effectiveness in handling deeper architectures.\n",
    "#### > InceptionV3 <br>\n",
    "![Inceptionv3 Architecture](inceptionv3.jpg)\n",
    " - a deep convolutional neural network architecture developed by Google's DeepMind team. It is an evolution of the original Inception architecture (also known as GoogLeNet). InceptionV3 is designed to improve computational efficiency while maintaining high accuracy in image recognition tasks. It achieves this by using a combination of 1x1, 3x3, and 5x5 convolutions in parallel to capture features at different scales. Additionally, it incorporates the concept of factorizing convolutions to reduce the number of parameters and computations required. InceptionV3 has been widely used in various applications and is known for its competitive performance on image classification tasks.\n",
    "#### > MobileNetV2 <br>\n",
    "![MobileNetv2 Architecture](mobilenetv2.png)\n",
    " - MobileNetV2 is a lightweight deep neural network architecture developed by Google. It is designed for efficient deployment on mobile and embedded devices, where computational resources are limited. MobileNetV2 achieves efficiency by employing depthwise separable convolutions, which split the standard convolution operation into separate depthwise and pointwise convolutions, reducing the number of computations required. This allows MobileNetV2 to be much faster and smaller in size compared to traditional architectures while maintaining reasonable accuracy on image classification tasks. Due to its efficiency, MobileNetV2 has found applications in various mobile and real-time vision-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "\n",
    "class ImageClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        img_size=224,\n",
    "        num_classes=4,\n",
    "        train_dir=\"../dataset/train\",\n",
    "        val_dir=\"../dataset/validation\",\n",
    "    ):\n",
    "        # Define the input size and number of classes\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Specify the pre-trained model to use\n",
    "        if model_name == \"VGG19\":\n",
    "            self.pretrained_model = VGG19(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                input_shape=(img_size, img_size, 3),\n",
    "            )\n",
    "        elif model_name == \"ResNet50\":\n",
    "            self.pretrained_model = ResNet50(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                input_shape=(img_size, img_size, 3),\n",
    "            )\n",
    "        elif model_name == \"InceptionV3\":\n",
    "            self.pretrained_model = InceptionV3(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                input_shape=(img_size, img_size, 3),\n",
    "            )\n",
    "        elif model_name == \"MobileNetV2\":\n",
    "            self.pretrained_model = MobileNetV2(\n",
    "                weights=\"imagenet\",\n",
    "                include_top=False,\n",
    "                input_shape=(img_size, img_size, 3),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid model name. \" +\n",
    "                \"Supported models are\" +\n",
    "                \"VGG19, ResNet50, InceptionV3, and MobileNetV2.\"\n",
    "            )\n",
    "\n",
    "        # Freeze the layers in the pre-trained model\n",
    "        for layer in self.pretrained_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add a new classifier on top\n",
    "        x = self.pretrained_model.output\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        predictions = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        # Define the new model\n",
    "        self.model = tf.keras.models.Model(\n",
    "            inputs=self.pretrained_model.input, outputs=predictions\n",
    "        )\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "            run_eagerly=True\n",
    "        )\n",
    "\n",
    "        # Define the data generators for training and validation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1.0 / 255,\n",
    "            rotation_range=20,\n",
    "            zoom_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "        )\n",
    "\n",
    "        val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "        self.train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=32,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "        self.val_generator = val_datagen.flow_from_directory(\n",
    "            val_dir,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=32,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "    def train(self, epochs):\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            steps_per_epoch=self.train_generator.samples//epochs,\n",
    "            epochs=10,\n",
    "            validation_data=self.val_generator,\n",
    "            validation_steps=self.val_generator.samples//epochs,\n",
    "        )\n",
    "\n",
    "    def save_model(self, model_filename):\n",
    "        # Save the trained model\n",
    "        self.model.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Trainer GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from PyQt5.QtWidgets import (\n",
    "    QApplication,\n",
    "    QMainWindow,\n",
    "    QTableView,\n",
    "    QWidget,\n",
    "    QVBoxLayout,\n",
    "    QHBoxLayout,\n",
    "    QLabel,\n",
    "    QPushButton,\n",
    "    QFileDialog,\n",
    "    QComboBox,\n",
    "    QPlainTextEdit,\n",
    "    QToolBar,\n",
    "    QToolButton,\n",
    "    QMenu\n",
    ")\n",
    "from PyQt5.QtGui import QPixmap\n",
    "from PyQt5.QtCore import Qt\n",
    "from PyQt5.QtGui import QStandardItemModel, QStandardItem, QTextCursor\n",
    "import shutil\n",
    "\n",
    "from trainer import ImageClassifierTrainer\n",
    "\n",
    "\n",
    "class MainWindow(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a table view\n",
    "        self.table_view = QTableView(self)\n",
    "        # self.setFixedSize(800, 600)\n",
    "        self.setWindowFlags(\n",
    "            Qt.Window\n",
    "            | Qt.CustomizeWindowHint\n",
    "            | Qt.WindowTitleHint\n",
    "            | Qt.WindowCloseButtonHint\n",
    "        )\n",
    "\n",
    "        # Create a data model for the table\n",
    "        self.model = QStandardItemModel(0, 2, self)\n",
    "        self.model.setHorizontalHeaderLabels([\"File Name\", \"Ripeness\"])\n",
    "        self.table_view.setModel(self.model)\n",
    "\n",
    "        # Create an image preview widget\n",
    "        self.image_label = QLabel(self)\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "        self.show_image(\"placeholder.jpg\")\n",
    "\n",
    "        # Create two buttons\n",
    "        self.button1 = QPushButton(\"Add Images\", self)\n",
    "        self.button2 = QPushButton(\"Train Models\", self)\n",
    "        self.button1.clicked.connect(self.add_images)\n",
    "        self.button2.clicked.connect(self.train_model)\n",
    "\n",
    "        # Create a horizontal layout to hold the image and buttons\n",
    "        image_layout = QVBoxLayout()\n",
    "        image_layout.addWidget(self.image_label)\n",
    "        image_layout.addWidget(self.button1)\n",
    "        image_layout.addWidget(self.button2)\n",
    "\n",
    "        # Create a horizontal layout to hold the table and image layouts\n",
    "        table_widget = QWidget(self)\n",
    "        layout = QHBoxLayout(table_widget)\n",
    "        layout.addWidget(self.table_view)\n",
    "        layout.addLayout(image_layout)\n",
    "\n",
    "        # Set the central widget of the main window\n",
    "        self.setCentralWidget(table_widget)\n",
    "\n",
    "        # Connect the clicked signal of the table view to a custom slot\n",
    "        self.table_view.clicked.connect(self.handle_table_click)\n",
    "\n",
    "        self.text_edit = QPlainTextEdit()\n",
    "        # Redirect terminal output to the log\n",
    "        sys.stdout = self\n",
    "        # Initialize the output buffer\n",
    "        self.buffer = \"\"\n",
    "        image_layout.addWidget(self.text_edit)\n",
    "        self.setContextMenuPolicy(Qt.CustomContextMenu)\n",
    "        self.customContextMenuRequested.connect(self.show_context_menu)\n",
    "    \n",
    "    def show_context_menu(self, position):\n",
    "        context_menu = QMenu(self)\n",
    "        action1 = context_menu.addAction(\"Delete\")\n",
    "\n",
    "        action1.triggered.connect(self.action1_triggered)\n",
    "\n",
    "        context_menu.exec(self.table_view.viewport().mapToGlobal(position))\n",
    "\n",
    "    def action1_triggered(self):\n",
    "        index = self.table_view.currentIndex()\n",
    "        if index.isValid():\n",
    "            self.table_view.model().removeRow(index.row())\n",
    "\n",
    "    def write(self, message):\n",
    "        # Append the message to the output buffer\n",
    "        self.buffer += message\n",
    "\n",
    "        # If a newline character is encountered, flush the buffer to the log\n",
    "        if \"\\n\" in message:\n",
    "            lines = self.buffer.split(\"\\n\")\n",
    "            for line in lines[:-1]:\n",
    "                # Write each line to the log\n",
    "                line = re.sub(r\"[^\\x20-\\x7E]+\", \"\", line)\n",
    "                self.text_edit.moveCursor(QTextCursor.End)\n",
    "                self.text_edit.insertPlainText(line + \"\\n\")\n",
    "            # Clear the buffer\n",
    "            self.buffer = lines[-1]\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "    def show_image(self, filename):\n",
    "        pixmap = QPixmap(filename)\n",
    "        pixmap = pixmap.scaledToHeight(200)\n",
    "        self.image_label.setPixmap(pixmap)\n",
    "\n",
    "    def add_images(self):\n",
    "        files, _ = QFileDialog.getOpenFileNames(\n",
    "            self, \"Open file\", \"\", \"Images (*.png *.xpm *.jpg *.bmp *.gif)\"\n",
    "        )\n",
    "        for i, file in enumerate(files):\n",
    "            self.show_image(file)\n",
    "            row = self.model.rowCount()\n",
    "            self.model.insertRow(row)\n",
    "            combo_box = QComboBox(self)\n",
    "            combo_box.addItems([\"Ripe\", \"Unripe\", \"Semi Ripe\", \"Overripe\"])\n",
    "            for column in range(1):\n",
    "                self.model.setItem(row, 0, QStandardItem(file))\n",
    "                self.table_view.setIndexWidget(self.model.index(row, 1), combo_box)\n",
    "\n",
    "    def handle_table_click(self, index):\n",
    "        # Get the selected row and column index\n",
    "        index = self.table_view.selectedIndexes()[0]\n",
    "        row = index.row()\n",
    "        self.show_image(self.model.item(row, 0).text())\n",
    "\n",
    "    def train_model(self):\n",
    "        num_rows = self.model.rowCount()\n",
    "\n",
    "        # Create the folders if they don't exist\n",
    "        for folder in [\"Ripe\", \"Unripe\", \"Semi Ripe\", \"Overripe\"]:\n",
    "            if not os.path.exists(f\"../dataset/train/{folder}\"):\n",
    "                print(f\"Creating {folder} folder for training\")\n",
    "                os.makedirs(f\"../dataset/train/{folder}\")\n",
    "            if not os.path.exists(f\"../dataset/validation/{folder}\"):\n",
    "                print(f\"Creating {folder} folder for validation\")\n",
    "                os.makedirs(f\"../dataset/validation/{folder}\")\n",
    "\n",
    "        for row in range(num_rows):\n",
    "            # Get the combo box widget for the current row\n",
    "            combo_box = self.table_view.indexWidget(self.model.index(row, 1))\n",
    "            # Get the selected item from the combo box\n",
    "            item_data = combo_box.currentText()\n",
    "            print(f\"{row}: {item_data} {self.model.item(row, 0).text()}\")\n",
    "            # get the file extension\n",
    "            file_extension = self.model.item(row, 0).text().split(\".\")[-1]\n",
    "            # Transfer the image to the appropriate folder\n",
    "            shutil.copy(\n",
    "                self.model.item(row, 0).text(),\n",
    "                f\"../dataset/train/{item_data}/image_{row}.{file_extension}\",\n",
    "            )\n",
    "            print(f\"copying to ../dataset/train/{item_data}/image_{row}.{file_extension}\")\n",
    "        # Train the model\n",
    "        trainer = ImageClassifierTrainer(\"VGG19\")\n",
    "        trainer.train(num_rows)\n",
    "        trainer.save_model(\"model.h5\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = MainWindow()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image\n",
    "from keras.applications.vgg19 import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "\n",
    "class VGG19CoffeeClassifier:\n",
    "    def __init__(self, pretrained=True, model_path=None):\n",
    "        if pretrained:\n",
    "            # Load the pre-trained VGG19 model without the top layers\n",
    "            self.base_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "            # Add new classification layers to the model\n",
    "            x = self.base_model.output\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "            x = Dense(1024, activation='relu')(x)\n",
    "            predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "            # Define the new model with the added classification layers\n",
    "            self.model = Model(inputs=self.base_model.input, outputs=predictions)\n",
    "\n",
    "            # Freeze the weights of the pre-trained layers\n",
    "            for layer in self.base_model.layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "            # Compile the model with a SGD optimizer and categorical crossentropy loss\n",
    "            self.model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        else:\n",
    "            # Load the custom model from the .h5 file\n",
    "            self.model = load_model(model_path)\n",
    "\n",
    "        # Define the image size for the model input\n",
    "        self.img_width, self.img_height = 224, 224\n",
    "\n",
    "        # Define the label dictionary\n",
    "        self.label_dict = {0: \"Unripe\", 1: \"Semi-ripe\", 2: \"Ripe\", 3: \"Overripe\"}\n",
    "\n",
    "    def classify(self, image_path):\n",
    "        # Load the image to classify\n",
    "        img = image.load_img(image_path, target_size=(self.img_width, self.img_height))\n",
    "\n",
    "        # Convert the image to an array and preprocess it for input into the model\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Make a prediction with the model\n",
    "        preds = self.model.predict(x)\n",
    "\n",
    "        # Decode the predictions and return the predicted label\n",
    "        pred_index = np.argmax(preds)\n",
    "        pred_label = self.label_dict[pred_index]\n",
    "        accuracy = preds[0][pred_index]\n",
    "        return {\"accuracy\": f\"{accuracy * 100}%\", \"label\": pred_label}\n",
    "\n",
    "\n",
    "# Usage\n",
    "# vgg_coffee = VGG19CoffeeClassifier(pretrained=True, model_path='my_model.h5')\n",
    "# image_path = 'coffee_berry.png'\n",
    "# pred_label = vgg_coffee.classify(image_path)\n",
    "# print('Predicted:', pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working POC"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
